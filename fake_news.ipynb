{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "caminho_corpus = 'Fake.br-Corpus/full_texts'\n",
    "\n",
    "dataset = pd.read_csv('Fake.br-Corpus\\preprocessed\\pre-processed.csv', sep=',')\n",
    "\n",
    "\n",
    "def pre_processar(dataframe: pd.DataFrame):\n",
    "    dataframe[\"label\"] = dataframe[\"label\"].apply(lambda x: 1.0 if x == \"true\" else 0.0)\n",
    "    dataframe.rename(columns={\"preprocessed_news\": \"text\"}, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "    \n",
    "dataframe_pre_processado = pre_processar(dataset)\n",
    "\n",
    "print(dataframe_pre_processado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataframe_pre_processado[\"text\"], dataframe_pre_processado[\"label\"], random_state=42)\n",
    "\n",
    "print(f\"Notícias falsas (treino): {y_train.value_counts().get(0.0, 0)}\")\n",
    "print(f\"Notícias verdadeiras (treino): {y_train.value_counts().get(1.0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "tokenizador = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "modelo = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FakeNewsDetector:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.tokenizador = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "        self.modelo = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)\n",
    "        \n",
    "        self.tokenizer_gemma = AutoTokenizer.from_pretrained('google/gemma-3-4b-it')\n",
    "        self.modelo_gemma = AutoModelForCausalLM.from_pretrained('google/gemma-3-4b-it')\n",
    "        self.gerador_contexto = pipeline('text-generation', model=self.modelo_gemma, tokenizer=self.tokenizer_gemma)\n",
    "\n",
    "    def gerar_contexto(self, texto_noticia):\n",
    "        prompt = f\"\"\"\n",
    "        Dado o teor desta notícia, gere um contexto local, geográfico e temporal \n",
    "        incluindo datas e acontecimentos importantes, mas sem repetir a notícia. Seja conciso e relevante:\n",
    "\n",
    "        {texto_noticia}\n",
    "        \"\"\"\n",
    "        \n",
    "        contexto = self.gerador_contexto(\n",
    "            prompt,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )[0]['generated_text']\n",
    "\n",
    "        return contexto.strip()\n",
    "\n",
    "    def processar_dados(self, dataframe):\n",
    "        dataframe['contexto'] = dataframe['text'].apply(self.gerar_contexto)\n",
    "        \n",
    "        dataframe['texto_ampliado'] = dataframe.apply(\n",
    "            lambda x: f\"CONTEXTO: {x['contexto']}\\n\\nNOTÍCIA: {x['text']}\", \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return dataframe\n",
    "\n",
    "    class NewsDataset(Dataset):\n",
    "        def __init__(self, textos, rotulos, tokenizador, max_len=256):\n",
    "            self.textos = textos\n",
    "            self.rotulos = rotulos\n",
    "            self.tokenizador = tokenizador\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.textos)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            encoding = self.tokenizador.encode_plus(\n",
    "                self.textos[idx],\n",
    "                max_length=self.max_len,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(self.rotulos[idx], dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    def treinar(self, X_train, y_train, X_val, y_val):\n",
    "        train_dataset = self.NewsDataset(X_train, y_train, self.tokenizador)\n",
    "        val_dataset = self.NewsDataset(X_val, y_val, self.tokenizador)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "        # Configuração de treino (com early stopping)\n",
    "        optimizer = torch.optim.AdamW(self.modelo.parameters(), lr=2e-5)\n",
    "        best_f1 = 0\n",
    "        for epoch in range(5):\n",
    "            self.modelo.train()\n",
    "            for batch in train_loader:\n",
    "                outputs = self.modelo(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Validação\n",
    "            val_metrics = self.avaliar(val_loader)\n",
    "            if val_metrics['f1'] > best_f1:\n",
    "                best_f1 = val_metrics['f1']\n",
    "                torch.save(self.modelo.state_dict(), 'melhor_modelo.pt')\n",
    "\n",
    "\n",
    "# Pipeline completo\n",
    "if __name__ == \"__main__\":\n",
    "    # Carregar o dataset\n",
    "    df = pd.read_csv('Fake.br-Corpus/preprocessed/pre-processed.csv')\n",
    "    \n",
    "    # Renomear as colunas para garantir a consistência com o código\n",
    "    df.rename(columns={\"preprocessed_news\": \"text\"}, inplace=True)\n",
    "    \n",
    "    # Mapear os rótulos 'true' e 'fake' para valores numéricos\n",
    "    df['label'] = df['label'].map({'true': 1, 'fake': 0})\n",
    "    \n",
    "    # Inicializar o detector de fake news\n",
    "    detector = FakeNewsDetector()\n",
    "\n",
    "    # Processar os dados, gerando os contextos\n",
    "    df_processado = detector.processar_dados(df)\n",
    "    \n",
    "    # Dividir o dataset em treino e validação\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df_processado['texto_ampliado'],\n",
    "        df_processado['label'],\n",
    "        test_size=0.2,\n",
    "        stratify=df_processado['label']\n",
    "    )\n",
    "    \n",
    "    # Treinar o modelo de classificação\n",
    "    detector.treinar(X_train.tolist(), y_train.tolist(), X_val.tolist(), y_val.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classificar_noticia(texto, modelo, tokenizador, dispositivo):\n",
    "#     modelo.eval()  \n",
    "#     encoding = tokenizador.encode_plus(\n",
    "#         texto,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=128,\n",
    "#         return_token_type_ids=False,\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         return_attention_mask=True,\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "#     input_ids = encoding['input_ids'].to(dispositivo)\n",
    "#     attention_mask = encoding['attention_mask'].to(dispositivo)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         saidas = modelo(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         _, predicao = torch.max(saidas.logits, dim=1)\n",
    "\n",
    "#     return 'Falsa' if predicao.item() == 1 else 'Verdadeira'\n",
    "\n",
    "# texto_novo = \"Alckmin diz que gay por ele PSDB “desembarca”, mas não explica se utilizará o aparelho do filme MIB. O governador de São Paulo, Geraldo Alckmin, assegurou nesta terça (28) que, se assumir a presidência do PSDB, como previsto, o partido “desembarca do governo Michel Temer”. “Eu acho que não tem nenhuma razão o continuar no governo. Já não é de hoje que penso assim. Mas as reformas vão continuar”, acrescentou, em entrevista ao jornalista José Luiz Datena, na Rádio Bandeirantes. Ele já foi escolhido para ser presidente nacional do PSDB após uma aliança tucana que busca mitigar o clima de caos absoluto no partido. Nomes como Marconi Perillo (GO) e o senador Tasso Jereissati (CE) já desistiram da presidência. O prefeito de Manaus, Arthur Virgílio, no entanto, não gostou do acordo. A atitude de “desembarque” de Alckmin tem a típica mania recente tucana de apelar ao pior tipo de oportunismo. Porém, como Alckmin fará para que as pessoas esquecem que o partido foi aliado do governo atual por tantos meses? Aliás, não fosse a parceria com o PSDB, dificilmente Dilma teria caído. Goste Alckmin ou não, eles estão anexados. A não ser que ele tenha providenciado aquela mesma tecnologia utilizada na série de filmes \"\n",
    "# resultado = classificar_noticia(texto_novo, modelo, tokenizador, dispositivo)\n",
    "# print(f\"A notícia é: {resultado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Notícias verdadeiras: {np.sum(y_train == 0)}\")\n",
    "print(f\"Notícias falsas: {np.sum(y_train == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto gerado: <think>\n",
      "Okay, so I'm trying to help this user who provided a query in Portuguese. The task is to generate additional context for a given news excerpt without repeating the content. From what I understand, the user wants me to create a coherent paragraph that includes information about the location and timing mentioned in the news but doesn't just rehash the same points.\n",
      "\n",
      "Looking at the news excerpt, it's about Katia Abreu talking about expulsion from the PMDB party and mentions of other political figures like Dilma. I need to extract relevant details without repeating what's already there. The user probably wants context that adds depth or background information related to the time and place mentioned in the news.\n",
      "\n",
      "I should focus on the political environment around 2015-2016, maybe touch on issues within PMDB at that time, and perhaps mention other parties like PT to provide a broader picture. It's important to connect Katia Abreu's actions to the larger political climate of the period, especially considering corruption scandals which might have influenced her decision.\n",
      "\n",
      "I need to make sure the context is concise, no more than one paragraph, and flows naturally without duplicating information from the original news piece. I should also use clear and formal language appropriate for a news-related context.\n",
      "</think>\n",
      "\n",
      "No final de 2015 e início de 2016, o cenário político brasileiro estava marcado por crises e rearranjos partidários, especialmente no contexto da Operação Lava-Jato e das consequentes denúncias envolvendo figuras do PMDB. O partido, historicamente ligado ao governo federal, passava por uma crise de identidade e divisões internas, o que levou muitos aliados a buscarem abrigo em outras legendas ou a se declararem independentes. Katia Abreu, senadora pelo PMDB, fez parte desse movimento, expressando insatisfação com a direção do partido e ameaçando romper definitivamente com a legenda. A época também foi marcada por debates sobre reformas políticas e ética na política, temas que ganharam destaque na imprensa.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'Fake.br-Corpus/preprocessed/pre-processed.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "texto_noticia = df.iloc[0]['preprocessed_news']\n",
    "\n",
    "def gerar_contexto(texto_noticia):\n",
    "    prompt = f\"\"\"\n",
    "    Dada a notícia abaixo, gere um contexto adicional relevante e coeso de no maximo um paragrafo. O contexto deve incluir \n",
    "    informações sobre o local e o momento descritos na notícia, mas sem repetir o conteúdo da notícia:\n",
    "\n",
    "    NOTÍCIA: {texto_noticia}\n",
    "    CONTEXTO:\n",
    "    \"\"\"\n",
    "    \n",
    "    resposta = ollama.chat(model=\"deepseek-r1:14b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "\n",
    "    contexto = resposta.message.content \n",
    "    return contexto.strip()\n",
    "\n",
    "contexto_gerado = gerar_contexto(texto_noticia)\n",
    "\n",
    "print(\"Contexto gerado:\", contexto_gerado)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
