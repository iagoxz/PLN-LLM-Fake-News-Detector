{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "caminho_corpus = 'Fake.br-Corpus/full_texts'\n",
    "\n",
    "dataset = pd.read_csv('Fake.br-Corpus\\preprocessed\\pre-processed.csv', sep=',')\n",
    "\n",
    "\n",
    "def pre_processar(dataframe: pd.DataFrame):\n",
    "    dataframe[\"label\"] = dataframe[\"label\"].apply(lambda x: 1.0 if x == \"true\" else 0.0)\n",
    "    dataframe.rename(columns={\"preprocessed_news\": \"text\"}, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "    \n",
    "dataframe_pre_processado = pre_processar(dataset)\n",
    "\n",
    "print(dataframe_pre_processado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataframe_pre_processado[\"text\"], dataframe_pre_processado[\"label\"], random_state=42)\n",
    "\n",
    "print(f\"Notícias falsas (treino): {y_train.value_counts().get(0.0, 0)}\")\n",
    "print(f\"Notícias verdadeiras (treino): {y_train.value_counts().get(1.0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "tokenizador = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "modelo = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FakeNewsDetector:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.tokenizador = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "        self.modelo = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)\n",
    "        \n",
    "        self.tokenizer_gemma = AutoTokenizer.from_pretrained('google/gemma-3-4b-it')\n",
    "        self.modelo_gemma = AutoModelForCausalLM.from_pretrained('google/gemma-3-4b-it')\n",
    "        self.gerador_contexto = pipeline('text-generation', model=self.modelo_gemma, tokenizer=self.tokenizer_gemma)\n",
    "\n",
    "    def gerar_contexto(self, texto_noticia):\n",
    "        prompt = f\"\"\"\n",
    "        Dado o teor desta notícia, gere um contexto local, geográfico e temporal \n",
    "        incluindo datas e acontecimentos importantes, mas sem repetir a notícia. Seja conciso e relevante:\n",
    "\n",
    "        {texto_noticia}\n",
    "        \"\"\"\n",
    "        \n",
    "        contexto = self.gerador_contexto(\n",
    "            prompt,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )[0]['generated_text']\n",
    "\n",
    "        return contexto.strip()\n",
    "\n",
    "    def processar_dados(self, dataframe):\n",
    "        dataframe['contexto'] = dataframe['text'].apply(self.gerar_contexto)\n",
    "        \n",
    "        dataframe['texto_ampliado'] = dataframe.apply(\n",
    "            lambda x: f\"CONTEXTO: {x['contexto']}\\n\\nNOTÍCIA: {x['text']}\", \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return dataframe\n",
    "\n",
    "    class NewsDataset(Dataset):\n",
    "        def __init__(self, textos, rotulos, tokenizador, max_len=256):\n",
    "            self.textos = textos\n",
    "            self.rotulos = rotulos\n",
    "            self.tokenizador = tokenizador\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.textos)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            encoding = self.tokenizador.encode_plus(\n",
    "                self.textos[idx],\n",
    "                max_length=self.max_len,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(self.rotulos[idx], dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    def treinar(self, X_train, y_train, X_val, y_val):\n",
    "        train_dataset = self.NewsDataset(X_train, y_train, self.tokenizador)\n",
    "        val_dataset = self.NewsDataset(X_val, y_val, self.tokenizador)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "        # Configuração de treino (com early stopping)\n",
    "        optimizer = torch.optim.AdamW(self.modelo.parameters(), lr=2e-5)\n",
    "        best_f1 = 0\n",
    "        for epoch in range(5):\n",
    "            self.modelo.train()\n",
    "            for batch in train_loader:\n",
    "                outputs = self.modelo(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Validação\n",
    "            val_metrics = self.avaliar(val_loader)\n",
    "            if val_metrics['f1'] > best_f1:\n",
    "                best_f1 = val_metrics['f1']\n",
    "                torch.save(self.modelo.state_dict(), 'melhor_modelo.pt')\n",
    "\n",
    "\n",
    "# Pipeline completo\n",
    "if __name__ == \"__main__\":\n",
    "    # Carregar o dataset\n",
    "    df = pd.read_csv('Fake.br-Corpus/preprocessed/pre-processed.csv')\n",
    "    \n",
    "    # Renomear as colunas para garantir a consistência com o código\n",
    "    df.rename(columns={\"preprocessed_news\": \"text\"}, inplace=True)\n",
    "    \n",
    "    # Mapear os rótulos 'true' e 'fake' para valores numéricos\n",
    "    df['label'] = df['label'].map({'true': 1, 'fake': 0})\n",
    "    \n",
    "    # Inicializar o detector de fake news\n",
    "    detector = FakeNewsDetector()\n",
    "\n",
    "    # Processar os dados, gerando os contextos\n",
    "    df_processado = detector.processar_dados(df)\n",
    "    \n",
    "    # Dividir o dataset em treino e validação\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df_processado['texto_ampliado'],\n",
    "        df_processado['label'],\n",
    "        test_size=0.2,\n",
    "        stratify=df_processado['label']\n",
    "    )\n",
    "    \n",
    "    # Treinar o modelo de classificação\n",
    "    detector.treinar(X_train.tolist(), y_train.tolist(), X_val.tolist(), y_val.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classificar_noticia(texto, modelo, tokenizador, dispositivo):\n",
    "#     modelo.eval()  \n",
    "#     encoding = tokenizador.encode_plus(\n",
    "#         texto,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=128,\n",
    "#         return_token_type_ids=False,\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         return_attention_mask=True,\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "#     input_ids = encoding['input_ids'].to(dispositivo)\n",
    "#     attention_mask = encoding['attention_mask'].to(dispositivo)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         saidas = modelo(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         _, predicao = torch.max(saidas.logits, dim=1)\n",
    "\n",
    "#     return 'Falsa' if predicao.item() == 1 else 'Verdadeira'\n",
    "\n",
    "# texto_novo = \"Alckmin diz que gay por ele PSDB “desembarca”, mas não explica se utilizará o aparelho do filme MIB. O governador de São Paulo, Geraldo Alckmin, assegurou nesta terça (28) que, se assumir a presidência do PSDB, como previsto, o partido “desembarca do governo Michel Temer”. “Eu acho que não tem nenhuma razão o continuar no governo. Já não é de hoje que penso assim. Mas as reformas vão continuar”, acrescentou, em entrevista ao jornalista José Luiz Datena, na Rádio Bandeirantes. Ele já foi escolhido para ser presidente nacional do PSDB após uma aliança tucana que busca mitigar o clima de caos absoluto no partido. Nomes como Marconi Perillo (GO) e o senador Tasso Jereissati (CE) já desistiram da presidência. O prefeito de Manaus, Arthur Virgílio, no entanto, não gostou do acordo. A atitude de “desembarque” de Alckmin tem a típica mania recente tucana de apelar ao pior tipo de oportunismo. Porém, como Alckmin fará para que as pessoas esquecem que o partido foi aliado do governo atual por tantos meses? Aliás, não fosse a parceria com o PSDB, dificilmente Dilma teria caído. Goste Alckmin ou não, eles estão anexados. A não ser que ele tenha providenciado aquela mesma tecnologia utilizada na série de filmes \"\n",
    "# resultado = classificar_noticia(texto_novo, modelo, tokenizador, dispositivo)\n",
    "# print(f\"A notícia é: {resultado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Notícias verdadeiras: {np.sum(y_train == 0)}\")\n",
    "print(f\"Notícias falsas: {np.sum(y_train == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"deepseek-r1:14b\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m contexto.strip()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Gerar o contexto para a primeira notícia\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m contexto_gerado = \u001b[43mgerar_contexto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto_noticia\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Exibir o contexto gerado\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mContexto gerado:\u001b[39m\u001b[33m\"\u001b[39m, contexto_gerado)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgerar_contexto\u001b[39m\u001b[34m(texto_noticia)\u001b[39m\n\u001b[32m     13\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33mDada a notícia abaixo, gere um contexto adicional relevante e coeso. O contexto deve incluir \u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33minformações sobre o local e o momento descritos na notícia, mas sem repetir o conteúdo da notícia:\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33mCONTEXTO:\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Chamar o modelo DeepSeek R1 14B para gerar o contexto\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m resposta = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek-r1:14b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m contexto = resposta[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m contexto.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iagom\\Desktop\\tcc\\PLN-LLM-Fake-News-Detector\\.venv\\Lib\\site-packages\\ollama\\_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    290\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    291\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    299\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iagom\\Desktop\\tcc\\PLN-LLM-Fake-News-Detector\\.venv\\Lib\\site-packages\\ollama\\_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iagom\\Desktop\\tcc\\PLN-LLM-Fake-News-Detector\\.venv\\Lib\\site-packages\\ollama\\_client.py:122\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model \"deepseek-r1:14b\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'Fake.br-Corpus/preprocessed/pre-processed.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "texto_noticia = df.iloc[0]['preprocessed_news']\n",
    "\n",
    "def gerar_contexto(texto_noticia):\n",
    "    prompt = f\"\"\"\n",
    "    Dada a notícia abaixo, gere um contexto adicional relevante e coeso. O contexto deve incluir \n",
    "    informações sobre o local e o momento descritos na notícia, mas sem repetir o conteúdo da notícia:\n",
    "\n",
    "    NOTÍCIA: {texto_noticia}\n",
    "    CONTEXTO:\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    resposta = ollama.chat(model=\"deepseek-r1:14b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    contexto = resposta['text']\n",
    "    return contexto.strip()\n",
    "\n",
    "#gerar contexto apenas para primeira noticia\n",
    "contexto_gerado = gerar_contexto(texto_noticia)\n",
    "\n",
    "print(\"Contexto gerado:\", contexto_gerado)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
